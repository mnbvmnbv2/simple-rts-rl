{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad8f7e8",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab59265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.rts.config import EnvConfig\n",
    "from src.rts.env import init_state, p1_step\n",
    "from src.rts.utils import assert_valid_state, get_legal_moves\n",
    "from src.rts.visualizaiton import visualize_board\n",
    "\n",
    "from flax import nnx\n",
    "from src.rl.pqn import Model\n",
    "import optax\n",
    "from src.rl.pqn import Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rl.pqn import train_minibatched\n",
    "\n",
    "width = 10\n",
    "height = 10\n",
    "config = EnvConfig(\n",
    "    num_players=2,\n",
    "    board_width = width,\n",
    "    board_height = height,\n",
    "    num_neutral_bases = 3,\n",
    "    num_neutral_troops_start = 5,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "params = Params(\n",
    "    num_iterations=50,\n",
    "    lr=4e-4,\n",
    "    gamma=0.99,\n",
    "    q_lambda=0.92,\n",
    "    num_envs=50,\n",
    "    num_steps=250,\n",
    "    update_epochs=1,\n",
    "    num_minibatches=4,\n",
    "    epsilon=0.3,\n",
    ")\n",
    "q_net = Model(width*height*4, 512, width*height*4, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(q_net, optax.adam(params.lr))\n",
    "\n",
    "q_net, losses, cum_returns = train_minibatched(q_net, optimizer, config, params)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(cum_returns)\n",
    "plt.title(\"Cumulative Returns\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(cum_returns, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1608e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the trained model to play a game\n",
    "rng_key = jax.random.PRNGKey(0)\n",
    "state = init_state(rng_key, config)\n",
    "rewards = []\n",
    "for i in range(200):\n",
    "    legal_mask = get_legal_moves(state, 0)\n",
    "    legal_mask = jnp.array(legal_mask.flatten())\n",
    "    print(q_net(state.board.flatten())* legal_mask)\n",
    "    action = jnp.argmax((q_net(state.board.flatten()) + 1000) * legal_mask)\n",
    "    # split action from int to array\n",
    "    # y, x, direction\n",
    "    action = jnp.array([action // (width*4), (action % (width*4))//4, action % 4])\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state, p1_reward = p1_step(state, subkey, config, action)\n",
    "    rewards.append(p1_reward)\n",
    "    assert_valid_state(state)\n",
    "    if i % 5 == 0:\n",
    "        visualize_board(state)\n",
    "    print(p1_reward)\n",
    "print(rewards)\n",
    "# for each step print the cumulative reward to the end from that step\n",
    "print(np.cumsum(rewards))\n",
    "plt.plot(np.cumsum(rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
