{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rts.config import EnvConfig\n",
    "from src.rts.env import Board, EnvState, init_state, random_step, p1_step\n",
    "from src.rts.utils import assert_valid_state, get_legal_moves\n",
    "from src.rts.visualizaiton import visualize_board\n",
    "from src.rl.pqn import Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "from src.rl.pqn import Model\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAX_CHECK_TRACER_LEAKS\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a game with random moves for 1000 steps\n",
    "# Visualize the board interactivly\n",
    "rng_key = jax.random.PRNGKey(3)\n",
    "config = EnvConfig(\n",
    "    num_players=4,\n",
    "    board_width = 10,\n",
    "    board_height = 10,\n",
    "    num_neutral_bases = 6,\n",
    "    num_neutral_troops_start = 10,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "state = init_state(rng_key, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state = random_step(state, subkey, config)\n",
    "    assert_valid_state(state)\n",
    "    if i % 1 == 0:\n",
    "        visualize_board(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=(\"config\",))\n",
    "def batched_step(states, rng_keys, config):\n",
    "    def single_step(state, key):\n",
    "        return random_step(state, key, config)\n",
    "\n",
    "    return jax.vmap(single_step)(states, rng_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we vmap\n",
    "N = 50\n",
    "rng_key = jax.random.PRNGKey(3)\n",
    "rng_keys = jax.random.split(rng_key, N)\n",
    "\n",
    "# Create the initial state for each game via vmap.\n",
    "batched_init_state = jax.vmap(lambda key: init_state(key, config))\n",
    "states = batched_init_state(rng_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    # For each parallel game, split its RNG key into two:\n",
    "    # keys_split will have shape (N, 2, key_shape).\n",
    "    keys_split = jax.vmap(lambda key: jax.random.split(key, 2))(rng_keys)\n",
    "    # Update rng_keys to the first half and use the second half as subkeys.\n",
    "    rng_keys = keys_split[:, 0]\n",
    "    subkeys = keys_split[:, 1]\n",
    "\n",
    "    # Take one step in parallel for all games.\n",
    "    states = batched_step(states, subkeys, config)\n",
    "\n",
    "    # Visualize and validate\n",
    "    if i % 250 == 0:\n",
    "        board = Board(\n",
    "            player_troops = states.board.player_troops[79],\n",
    "            neutral_troops = states.board.neutral_troops[79],\n",
    "            bases = states.board.bases[79],\n",
    "        )\n",
    "        single_state = EnvState(board = board)\n",
    "        assert_valid_state(single_state)\n",
    "        visualize_board(single_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(3)\n",
    "width = 10\n",
    "height = 10\n",
    "config = EnvConfig(\n",
    "    num_players=4,\n",
    "    board_width = width,\n",
    "    board_height = height,\n",
    "    num_neutral_bases = 3,\n",
    "    num_neutral_troops_start = 5,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "model = Model(width*height*(config.num_players + 2), 256, width*height*4, rngs=nnx.Rngs(0))\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = init_state(rng_key, config)\n",
    "for i in range(50):\n",
    "    flat_state = jnp.array(state.board.flatten())\n",
    "    legal_mask = get_legal_moves(state, 0)\n",
    "    legal_mask = jnp.array(legal_mask.flatten())\n",
    "    action = jnp.argmax((model(flat_state) + 10) * legal_mask)\n",
    "    # split action from int to array\n",
    "    # y, x, direction\n",
    "    action = jnp.array([action // (width*4), (action % (width*4))//4, action % 4])\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state, p1_reward = p1_step(state, subkey, config, action)\n",
    "    assert_valid_state(state)\n",
    "    if i % 5 == 0:\n",
    "        visualize_board(state)\n",
    "    print(p1_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_state = state.board.flatten()\n",
    "legal_mask = get_legal_moves(state, 0).flatten()\n",
    "q_net_action = jnp.argmax((model(flat_state) + 1000) * legal_mask)\n",
    "q_net_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rl.pqn import single_rollout\n",
    "params = Params(\n",
    "    num_iterations=100,\n",
    "    lr=2e-4,\n",
    "    gamma=0.75,\n",
    "    q_lambda=0.95,\n",
    "    num_envs=2048,\n",
    "    num_steps=250,\n",
    "    update_epochs=3,\n",
    "    num_minibatches=10,\n",
    "    epsilon=0.09,\n",
    ")\n",
    "q_net = Model(width*height*(config.num_players + 2), 256, width*height*4, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(q_net, optax.adam(params.lr))\n",
    "\n",
    "obs_buffer, actions_buffer, rewards_buffer, done_buffer, next_obs_buffer, cum_return = single_rollout(rng_key, config, q_net, params.num_steps, params.epsilon)\n",
    "actions_buffer, done_buffer, rewards_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rl.pqn import q_lambda_return\n",
    "\n",
    "returns = q_lambda_return(q_net, rewards_buffer, done_buffer, next_obs_buffer, params.gamma, params.q_lambda)\n",
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rl.pqn import train_step\n",
    "\n",
    "train_step(q_net, optimizer, obs_buffer, actions_buffer, returns)\n",
    "\n",
    "print(jnp.take_along_axis(q_net(obs_buffer), actions_buffer[:, None], axis=1).squeeze())\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q_net: Model, optimizer: nnx.Optimizer, config: EnvConfig, params: Params):\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    losses = []\n",
    "    for iteration in range(params.num_iterations):\n",
    "        rng_key, rollout_key = jax.random.split(rng_key)\n",
    "        rollout = single_rollout(rollout_key, config, q_net, params.num_steps, params.epsilon)\n",
    "        obs_buffer, actions_buffer, rewards_buffer, done_buffer, next_obs_buffer, cum_return = rollout\n",
    "\n",
    "        returns = q_lambda_return(q_net, rewards_buffer, done_buffer, next_obs_buffer, params.gamma, params.q_lambda)\n",
    "\n",
    "        for epoch in range(params.update_epochs):\n",
    "            loss = train_step(q_net, optimizer, obs_buffer, actions_buffer, returns)\n",
    "            losses.append(loss)\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration} - Loss: {loss}\")\n",
    "\n",
    "    # evaluate\n",
    "    state = init_state(rng_key, config)\n",
    "    q_net(state.board.flatten())\n",
    "\n",
    "    plt.plot(losses)\n",
    "\n",
    "    return q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 10\n",
    "height = 10\n",
    "config = EnvConfig(\n",
    "    num_players=2,\n",
    "    board_width = width,\n",
    "    board_height = height,\n",
    "    num_neutral_bases = 3,\n",
    "    num_neutral_troops_start = 5,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "params = Params(\n",
    "    num_iterations=100,\n",
    "    lr=2e-4,\n",
    "    gamma=0.75,\n",
    "    q_lambda=0.95,\n",
    "    num_envs=2048,\n",
    "    num_steps=250,\n",
    "    update_epochs=3,\n",
    "    num_minibatches=10,\n",
    "    epsilon=0.09,\n",
    ")\n",
    "q_net = Model(width*height*4, 256, width*height*4, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(q_net, optax.adam(params.lr))\n",
    "\n",
    "q_net = train(q_net, optimizer, config, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the trained model to play a game\n",
    "state = init_state(rng_key, config)\n",
    "rewards = []\n",
    "for i in range(200):\n",
    "    legal_mask = get_legal_moves(state, 0)\n",
    "    legal_mask = jnp.array(legal_mask.flatten())\n",
    "    print(q_net(state.board.flatten())* legal_mask)\n",
    "    action = jnp.argmax((q_net(state.board.flatten()) + 1000) * legal_mask)\n",
    "    # split action from int to array\n",
    "    # y, x, direction\n",
    "    action = jnp.array([action // (width*4), (action % (width*4))//4, action % 4])\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state, p1_reward = p1_step(state, subkey, config, action)\n",
    "    rewards.append(p1_reward)\n",
    "    assert_valid_state(state)\n",
    "    if i % 5 == 0:\n",
    "        visualize_board(state)\n",
    "# for each step print the cumulative reward to the end from that step\n",
    "print(np.cumsum(rewards))\n",
    "plt.plot(np.cumsum(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PQN with vmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rl.pqn import train_minibatched\n",
    "\n",
    "width = 10\n",
    "height = 10\n",
    "config = EnvConfig(\n",
    "    num_players=2,\n",
    "    board_width = width,\n",
    "    board_height = height,\n",
    "    num_neutral_bases = 3,\n",
    "    num_neutral_troops_start = 5,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "params = Params(\n",
    "    num_iterations=2000,\n",
    "    lr=2e-4,\n",
    "    gamma=0.75,\n",
    "    q_lambda=0.95,\n",
    "    num_envs=512,\n",
    "    num_steps=250,\n",
    "    update_epochs=3,\n",
    "    num_minibatches=10,\n",
    "    epsilon=0.09,\n",
    ")\n",
    "q_net = Model(width*height*4, 512, width*height*4, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(q_net, optax.adam(params.lr))\n",
    "\n",
    "q_net, losses, cum_returns = train_minibatched(q_net, optimizer, config, params)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(cum_returns)\n",
    "plt.title(\"Cumulative Returns\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(cum_returns, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the trained model to play a game\n",
    "rng_key = jax.random.PRNGKey(3)\n",
    "state = init_state(rng_key, config)\n",
    "rewards = []\n",
    "for i in range(300):\n",
    "    legal_mask = get_legal_moves(state, 0)\n",
    "    legal_mask = jnp.array(legal_mask.flatten())\n",
    "    action = jnp.argmax((q_net(state.board.flatten()) + 1000) * legal_mask)\n",
    "    # split action from int to array\n",
    "    # y, x, direction\n",
    "    action = jnp.array([action // (width*4), (action % (width*4))//4, action % 4])\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state, p1_reward = p1_step(state, subkey, config, action)\n",
    "    rewards.append(p1_reward)\n",
    "    visualize_board(state)\n",
    "# for each step print the cumulative reward to the end from that step\n",
    "plt.plot(np.cumsum(rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
