{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import nnx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rts.config import EnvConfig\n",
    "from src.rts.env import Board, EnvState, init_state\n",
    "from src.rts.utils import assert_valid_state, get_legal_moves, random_step, p1_step\n",
    "from src.rts.visualizaiton import visualize_board\n",
    "from src.rl.pqn import Params\n",
    "from src.rl.model import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAX_CHECK_TRACER_LEAKS\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a game with random moves for 1000 steps\n",
    "# Visualize the board interactivly\n",
    "rng_key = jax.random.PRNGKey(3)\n",
    "config = EnvConfig(\n",
    "    num_players=4,\n",
    "    board_width = 10,\n",
    "    board_height = 10,\n",
    "    num_neutral_bases = 6,\n",
    "    num_neutral_troops_start = 10,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "state = init_state(rng_key, config)\n",
    "\n",
    "for i in range(10):\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state = random_step(state, subkey, config)\n",
    "    assert_valid_state(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched Random (Vmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=(\"config\",))\n",
    "def batched_step(states, rng_keys, config):\n",
    "    def single_step(state, key):\n",
    "        return random_step(state, key, config)\n",
    "\n",
    "    return jax.vmap(single_step)(states, rng_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we vmap\n",
    "N = 50\n",
    "rng_key = jax.random.PRNGKey(3)\n",
    "rng_keys = jax.random.split(rng_key, N)\n",
    "\n",
    "# Create the initial state for each game via vmap.\n",
    "batched_init_state = jax.vmap(lambda key: init_state(key, config))\n",
    "states = batched_init_state(rng_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    # For each parallel game, split its RNG key into two:\n",
    "    # keys_split will have shape (N, 2, key_shape).\n",
    "    keys_split = jax.vmap(lambda key: jax.random.split(key, 2))(rng_keys)\n",
    "    # Update rng_keys to the first half and use the second half as subkeys.\n",
    "    rng_keys = keys_split[:, 0]\n",
    "    subkeys = keys_split[:, 1]\n",
    "\n",
    "    # Take one step in parallel for all games.\n",
    "    states = batched_step(states, subkeys, config)\n",
    "\n",
    "    # Visualize one game and validate\n",
    "    if i % 250 == 0:\n",
    "        board = Board(\n",
    "            player_troops = states.board.player_troops[79],\n",
    "            neutral_troops = states.board.neutral_troops[79],\n",
    "            bases = states.board.bases[79],\n",
    "        )\n",
    "        single_state = EnvState(board = board)\n",
    "        assert_valid_state(single_state)\n",
    "        visualize_board(single_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(3)\n",
    "width = 10\n",
    "height = 10\n",
    "config = EnvConfig(\n",
    "    num_players=4,\n",
    "    board_width = width,\n",
    "    board_height = height,\n",
    "    num_neutral_bases = 3,\n",
    "    num_neutral_troops_start = 5,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "model = MLP(width*height*(config.num_players + 2), [256, 256], width*height*4, rngs=nnx.Rngs(0))\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_state = state.board.flatten()\n",
    "legal_mask = get_legal_moves(state, 0).flatten()\n",
    "q_net_action = jnp.argmax((model(flat_state) + 1000) * legal_mask) # FIXME: should pick moves in a better way\n",
    "q_net_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = init_state(rng_key, config)\n",
    "for i in range(50):\n",
    "    flat_state = jnp.array(state.board.flatten())\n",
    "    legal_mask = get_legal_moves(state, 0)\n",
    "    legal_mask = jnp.array(legal_mask.flatten())\n",
    "    action = jnp.argmax((model(flat_state) + 1000) * legal_mask)\n",
    "    # split action from int to array\n",
    "    # y, x, direction\n",
    "    action = jnp.array([action // (width*4), (action % (width*4))//4, action % 4])\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state, p1_reward = p1_step(state, subkey, config, action)\n",
    "    assert_valid_state(state)\n",
    "    if i % 5 == 0:\n",
    "        visualize_board(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rl.pqn import single_rollout\n",
    "from src.rl.pqn import q_lambda_return\n",
    "from src.rl.pqn import train_step\n",
    "params = Params(\n",
    "    num_iterations=30,\n",
    "    lr=2e-4,\n",
    "    gamma=0.75,\n",
    "    q_lambda=0.95,\n",
    "    num_envs=2048,\n",
    "    num_steps=250,\n",
    "    update_epochs=3,\n",
    "    num_minibatches=10,\n",
    "    epsilon=0.09,\n",
    ")\n",
    "q_net = MLP(width*height*(config.num_players + 2), [256, 256], width*height*4, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(q_net, optax.adam(params.lr))\n",
    "\n",
    "obs_buffer, actions_buffer, rewards_buffer, done_buffer, next_obs_buffer, cum_return = single_rollout(rng_key, config, q_net, params.num_steps, params.epsilon)\n",
    "actions_buffer, done_buffer, rewards_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q_net: nnx.Module, optimizer: nnx.Optimizer, config: EnvConfig, params: Params):\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    losses = []\n",
    "    for iteration in range(params.num_iterations):\n",
    "        rng_key, rollout_key = jax.random.split(rng_key)\n",
    "        rollout = single_rollout(rollout_key, config, q_net, params.num_steps, params.epsilon)\n",
    "        obs_buffer, actions_buffer, rewards_buffer, done_buffer, next_obs_buffer, cum_return = rollout\n",
    "\n",
    "        returns = q_lambda_return(q_net, rewards_buffer, done_buffer, next_obs_buffer, params.gamma, params.q_lambda)\n",
    "\n",
    "        for epoch in range(params.update_epochs):\n",
    "            loss = train_step(q_net, optimizer, obs_buffer, actions_buffer, returns)\n",
    "            losses.append(loss)\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration} - Loss: {loss}\")\n",
    "\n",
    "    # evaluate\n",
    "    state = init_state(rng_key, config)\n",
    "    q_net(state.board.flatten())\n",
    "\n",
    "    plt.plot(losses)\n",
    "\n",
    "    return q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 10\n",
    "height = 10\n",
    "config = EnvConfig(\n",
    "    num_players=2,\n",
    "    board_width = width,\n",
    "    board_height = height,\n",
    "    num_neutral_bases = 3,\n",
    "    num_neutral_troops_start = 5,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "params = Params(\n",
    "    num_iterations=25,\n",
    "    lr=2e-4,\n",
    "    gamma=0.75,\n",
    "    q_lambda=0.95,\n",
    "    num_envs=512,\n",
    "    num_steps=250,\n",
    "    update_epochs=3,\n",
    "    num_minibatches=10,\n",
    "    epsilon=0.09,\n",
    ")\n",
    "q_net = MLP(width*height*4, [256, 256], width*height*4, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(q_net, optax.adam(params.lr))\n",
    "\n",
    "q_net = train(q_net, optimizer, config, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the trained model to play a game\n",
    "state = init_state(rng_key, config)\n",
    "rewards = []\n",
    "for i in range(40):\n",
    "    legal_mask = get_legal_moves(state, 0)\n",
    "    legal_mask = jnp.array(legal_mask.flatten())\n",
    "    print(q_net(state.board.flatten())* legal_mask)\n",
    "    action = jnp.argmax((q_net(state.board.flatten()) + 1000) * legal_mask)\n",
    "    # split action from int to array\n",
    "    # y, x, direction\n",
    "    action = jnp.array([action // (width*4), (action % (width*4))//4, action % 4])\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state, p1_reward = p1_step(state, subkey, config, action)\n",
    "    rewards.append(p1_reward)\n",
    "    assert_valid_state(state)\n",
    "    if i % 5 == 0:\n",
    "        visualize_board(state)\n",
    "# for each step print the cumulative reward to the end from that step\n",
    "plt.plot(np.cumsum(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PQN with vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rl.pqn import train_minibatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 10\n",
    "height = 10\n",
    "config = EnvConfig(\n",
    "    num_players=2,\n",
    "    board_width = width,\n",
    "    board_height = height,\n",
    "    num_neutral_bases = 3,\n",
    "    num_neutral_troops_start = 5,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "params = Params(\n",
    "    num_iterations=20,\n",
    "    lr=2e-5,\n",
    "    gamma=0.8,\n",
    "    q_lambda=0.95,\n",
    "    num_envs=512,\n",
    "    num_steps=250,\n",
    "    update_epochs=3,\n",
    "    num_minibatches=8,\n",
    "    epsilon=0.005,\n",
    ")\n",
    "q_net = MLP(width*height*4, [512, 512], width*height*4, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(q_net, optax.adam(params.lr))\n",
    "q_net, losses, cum_returns, timings = train_minibatched(q_net, optimizer, config, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize timings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "timings = {k: np.array(v) for k, v in timings.items()}\n",
    "plt.figure(figsize=(12, 6))\n",
    "for key, values in timings.items():\n",
    "    plt.plot(values[:6], label=key)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Training Timings')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timings = {k: np.array(v) for k, v in timings.items()}\n",
    "plt.figure(figsize=(12, 6))\n",
    "for key, values in timings.items():\n",
    "    plt.plot(values[:2], label=key)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Training Timings')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add up all timings\n",
    "total_timings = {key: np.sum(values[1:]) for key, values in timings.items()}\n",
    "print(\"Total Timings:\")\n",
    "for key, value in total_timings.items():\n",
    "    print(f\"{key}: {value:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(total_timings.values())  # Should be close to the total time taken for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rl.pqn import train_minibatched\n",
    "from src.rl.eval import evaluate_batch\n",
    "\n",
    "width = 10\n",
    "height = 10\n",
    "config = EnvConfig(\n",
    "    num_players=2,\n",
    "    board_width = width,\n",
    "    board_height = height,\n",
    "    num_neutral_bases = 3,\n",
    "    num_neutral_troops_start = 5,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "params = Params(\n",
    "    num_iterations=20,\n",
    "    lr=8e-5,\n",
    "    gamma=0.9,\n",
    "    q_lambda=0.95,\n",
    "    num_envs=512,\n",
    "    num_steps=250,\n",
    "    update_epochs=3,\n",
    "    num_minibatches=8,\n",
    "    epsilon=0.005,\n",
    ")\n",
    "q_net = MLP(width*height*4, [512, 512], width*height*4, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(q_net, optax.adam(params.lr))\n",
    "\n",
    "with jax.profiler.trace(\"/tmp/train_minibatched\", create_perfetto_link=True):\n",
    "    q_net, losses, cum_returns, timings = train_minibatched(q_net, optimizer, config, params)\n",
    "     \n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.mean(cum_returns, axis=0))\n",
    "plt.title(\"Cumulative Returns\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "output = float(np.mean(evaluate_batch(q_net, config, jax.random.PRNGKey(0), batch_size=100, num_steps=250)))\n",
    "print(f\"Evaluation output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(2)\n",
    "state = init_state(rng_key, config)\n",
    "rewards = []\n",
    "for i in range(25):\n",
    "    legal_mask = get_legal_moves(state, 0)\n",
    "    legal_mask = jnp.array(legal_mask.flatten())\n",
    "    action = jnp.argmax((q_net(state.board.flatten()) + 1000) * legal_mask)\n",
    "    action = jnp.array([action // (width*4), (action % (width*4))//4, action % 4])\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state, p1_reward = p1_step(state, subkey, config, action)\n",
    "    rewards.append(p1_reward)\n",
    "    if i % 5 == 0:\n",
    "        visualize_board(state)\n",
    "plt.plot(np.cumsum(rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
