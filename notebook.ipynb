{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.lax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rts.config import EnvConfig\n",
    "from src.rts.env import Board, EnvState, init_state, move, reinforce_troops, is_done\n",
    "from src.rts.utils import assert_valid_state, get_legal_moves, fixed_argwhere\n",
    "\n",
    "from src.rts.visualizaiton import visualize_board\n",
    "\n",
    "from src.main import step, batched_step, p1_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAX_CHECK_TRACER_LEAKS\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a game with random moves for 1000 steps\n",
    "# Visualize the board interactivly\n",
    "rng_key = jax.random.PRNGKey(3)\n",
    "config = EnvConfig(\n",
    "    board_width = 10,\n",
    "    board_height = 10,\n",
    "    num_neutral_bases = 6,\n",
    "    num_neutral_troops_start = 10,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "state = init_state(rng_key, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state, p1_reward = step(state, subkey, config)\n",
    "    assert_valid_state(state)\n",
    "    if i % 1 == 0:\n",
    "        visualize_board(state)\n",
    "    print(p1_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import optax\n",
    "\n",
    "class Model(nnx.Module):\n",
    "    def __init__(self, in_dim, mid_dim, out_dim, rngs: nnx.Rngs):\n",
    "        self.lin_in = nnx.Linear(in_dim, mid_dim, rngs=rngs)\n",
    "        self.layer_norm = nnx.LayerNorm(mid_dim, rngs=rngs)\n",
    "        self.lin_out = nnx.Linear(mid_dim, out_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nnx.relu(self.layer_norm(self.lin_in(x)))\n",
    "        # x = nnx.relu(self.lin_in(x))\n",
    "        return self.lin_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(3)\n",
    "width = 10\n",
    "height = 10\n",
    "model = Model(width*height*4, 256, width*height*4, rngs=nnx.Rngs(0))\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EnvConfig(\n",
    "    board_width = width,\n",
    "    board_height = height,\n",
    "    num_neutral_bases = 3,\n",
    "    num_neutral_troops_start = 5,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "\n",
    "state = init_state(rng_key, config)\n",
    "for i in range(50):\n",
    "    flat_state = jnp.array(state.board.flatten())\n",
    "    legal_mask = get_legal_moves(state, 0)\n",
    "    legal_mask = jnp.array(legal_mask.flatten())\n",
    "    action = jnp.argmax((model(flat_state) + 10) * legal_mask)\n",
    "    # split action from int to array\n",
    "    # y, x, direction\n",
    "    action = jnp.array([action // (width*4), (action % (width*4))//4, action % 4])\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state, p1_reward = p1_step(state, subkey, config, action)\n",
    "    assert_valid_state(state)\n",
    "    if i % 5 == 0:\n",
    "        visualize_board(state)\n",
    "    print(p1_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we vmap\n",
    "N = 50\n",
    "rng_key = jax.random.PRNGKey(3)\n",
    "rng_keys = jax.random.split(rng_key, N)\n",
    "\n",
    "# Create the initial state for each game via vmap.\n",
    "batched_init_state = jax.vmap(lambda key: init_state(key, config))\n",
    "states = batched_init_state(rng_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    # For each parallel game, split its RNG key into two:\n",
    "    # keys_split will have shape (N, 2, key_shape).\n",
    "    keys_split = jax.vmap(lambda key: jax.random.split(key, 2))(rng_keys)\n",
    "    # Update rng_keys to the first half and use the second half as subkeys.\n",
    "    rng_keys = keys_split[:, 0]\n",
    "    subkeys = keys_split[:, 1]\n",
    "\n",
    "    # Take one step in parallel for all games.\n",
    "    states, p1_rewards = batched_step(states, subkeys, config)\n",
    "\n",
    "    # Visualize and validate\n",
    "    if i % 250 == 0:\n",
    "        print(p1_rewards)\n",
    "        board = Board(\n",
    "            player_1_troops = states.board.player_1_troops[79],\n",
    "            player_2_troops = states.board.player_2_troops[79],\n",
    "            neutral_troops = states.board.neutral_troops[79],\n",
    "            bases = states.board.bases[79],\n",
    "        )\n",
    "        single_state = EnvState(board = board)\n",
    "        assert_valid_state(single_state)\n",
    "        visualize_board(single_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def time_block(label: str, timing_dict: dict[str, float]):\n",
    "    start_time = time.time()\n",
    "    yield\n",
    "    end_time = time.time()\n",
    "    timing_dict[label] = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Params:\n",
    "    num_iterations: int\n",
    "    lr: float\n",
    "    gamma: float\n",
    "    q_lambda: float\n",
    "    num_envs: int\n",
    "    num_steps: int\n",
    "    update_epochs: int\n",
    "    num_minibatches: int\n",
    "    epsilon: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_state = state.board.flatten()\n",
    "legal_mask = get_legal_moves(state, 0).flatten()\n",
    "q_net_action = jnp.argmax((model(flat_state) + 1000) * legal_mask)\n",
    "q_net_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params(\n",
    "    num_iterations=100,\n",
    "    lr=6e-4,\n",
    "    gamma=0.99,\n",
    "    q_lambda=0.65,\n",
    "    num_envs=2048,\n",
    "    num_steps=26,\n",
    "    update_epochs=4,\n",
    "    num_minibatches=4,\n",
    "    epsilon=0.25,\n",
    ")\n",
    "q_net = Model(width*height*4, 256, width*height*4, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(q_net, optax.adam(params.lr))\n",
    "\n",
    "# @nnx.jit\n",
    "@functools.partial(nnx.jit, static_argnums=(1,3))\n",
    "def single_rollout(rng_key, config: EnvConfig, model: Model, params: Params):\n",
    "    state = init_state(rng_key, config)\n",
    "    \n",
    "    def policy_step(carry, _):\n",
    "        state, rng_key, cum_reward = carry\n",
    "        \n",
    "        flat_state = state.board.flatten()\n",
    "        legal_mask = get_legal_moves(state, 0).flatten()\n",
    "        \n",
    "        logits = model(flat_state)\n",
    "        # choose the action with the highest Q-value that is also legal\n",
    "        q_net_action = jnp.argmax((logits + 1000) * legal_mask)\n",
    "        # epsilon-greedy exploration\n",
    "        legal_actions, num_actions = fixed_argwhere(\n",
    "            legal_mask, max_actions=state.board.width * state.board.height * 4\n",
    "        )\n",
    "        rng_key, subkey = jax.random.split(rng_key)\n",
    "        action_idx = jax.random.randint(subkey, (), 0, num_actions)\n",
    "        explore_action = jnp.take(legal_actions, action_idx, axis=0)[0]\n",
    "\n",
    "        action = jax.lax.cond(\n",
    "            jax.random.bernoulli(rng_key, params.epsilon),\n",
    "            lambda _: explore_action,\n",
    "            lambda _: q_net_action,\n",
    "            operand=None,\n",
    "        )\n",
    "        \n",
    "        # Split the scalar action into (row, col, direction) components.\n",
    "        action_split = jnp.array([\n",
    "            action // (config.board_width * 4),\n",
    "            (action % (config.board_width * 4)) // 4,\n",
    "            action % 4\n",
    "        ])\n",
    "        \n",
    "        rng_key, subkey = jax.random.split(rng_key)\n",
    "        next_state, p1_reward = p1_step(state, subkey, config, action_split)\n",
    "        \n",
    "        new_cum_reward = cum_reward + p1_reward\n",
    "\n",
    "        done = is_done(next_state)\n",
    "        \n",
    "        # The buffer (y) collects: observation (state.board), action, reward, done, next observation.\n",
    "        y = (state.board.flatten(), action, p1_reward, done, next_state.board.flatten())\n",
    "        \n",
    "        # The new carry is the updated state, RNG key, and cumulative reward.\n",
    "        return (next_state, rng_key, new_cum_reward), y\n",
    "\n",
    "    (final_state, final_rng, cum_return), scan_out = jax.lax.scan(\n",
    "        policy_step,\n",
    "        (state, rng_key, jnp.array(0.0)),\n",
    "        None,\n",
    "        params.num_steps\n",
    "    )\n",
    "    obs_buffer, actions_buffer, rewards_buffer, done_buffer, next_obs_buffer = scan_out\n",
    "    return obs_buffer, actions_buffer, rewards_buffer, done_buffer, next_obs_buffer, cum_return\n",
    "\n",
    "obs_buffer, actions_buffer, rewards_buffer, done_buffer, next_obs_buffer, cum_return = single_rollout(rng_key, config, q_net, params)\n",
    "actions_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(nnx.jit, static_argnums=(4,))\n",
    "def q_lambda_return(q_net: Model, rewards_buffer: jnp.ndarray, done_buffer: jnp.ndarray, next_obs_buffer: jnp.ndarray, params: Params):\n",
    "    # Compute Q-values for the next observations via vectorized max.\n",
    "    # This returns an array of shape (num_steps,)\n",
    "    q_values = jax.vmap(lambda obs: jnp.max(q_net(obs), axis=-1))(next_obs_buffer)\n",
    "\n",
    "    # For the final step, compute the return as:\n",
    "    # returns[-1] = rewards[-1] + gamma * q_value[-1] * (1 - done[-1])\n",
    "    returns_last = rewards_buffer[-1] + params.gamma * q_values[-1] * (1.0 - done_buffer[-1])\n",
    "    \n",
    "    # For timesteps 0,...,num_steps-2 we use:\n",
    "    # returns[t] = rewards[t] + gamma * (q_lambda * returns[t+1] +\n",
    "    #                                    (1 - q_lambda) * q_value[t+1] * (1 - done[t+1]))\n",
    "    # To compute this in reverse, we reverse the arrays (excluding the last step).\n",
    "    rewards_rev = rewards_buffer[:-1][::-1]\n",
    "    dones_rev = done_buffer[1:][::-1]\n",
    "    next_vals_rev = q_values[1:][::-1]\n",
    "\n",
    "    def scan_fn(next_return, inputs):\n",
    "        reward, done, next_value = inputs\n",
    "        nextnonterminal = 1.0 - done\n",
    "        current_return = reward + params.gamma * (\n",
    "            params.q_lambda * next_return +\n",
    "            (1 - params.q_lambda) * next_value * nextnonterminal\n",
    "        )\n",
    "        return current_return, current_return\n",
    "\n",
    "    # The scan will traverse the reversed sequences.\n",
    "    # Its initial carry is the last return (for t = num_steps - 1)\n",
    "    _, returns_rev_scan = jax.lax.scan(\n",
    "        scan_fn,\n",
    "        returns_last,\n",
    "        (rewards_rev, dones_rev, next_vals_rev)\n",
    "    )\n",
    "    \n",
    "    # Flip the scanned returns back to the original order.\n",
    "    returns_first_part = returns_rev_scan[::-1]\n",
    "    # Append the final return computed above.\n",
    "    full_returns = jnp.concatenate([returns_first_part, jnp.array([returns_last])], axis=0)\n",
    "    \n",
    "    return full_returns\n",
    "\n",
    "returns = q_lambda_return(q_net, rewards_buffer, done_buffer, next_obs_buffer, params)\n",
    "returns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = params.num_steps * params.num_envs // params.num_minibatches\n",
    "\n",
    "# @functools.partial(jax.jit, static_argnums=(0,1))\n",
    "@nnx.jit\n",
    "def train_step(q_net: Model, optimizer: nnx.Optimizer, observations: jnp.ndarray, actions: jnp.ndarray, returns: jnp.ndarray):\n",
    "    def loss_fn(q_net):\n",
    "        q_values = q_net(observations)\n",
    "        acted_q_values = jnp.take_along_axis(q_values, actions[:, None], axis=1).squeeze()\n",
    "        return ((acted_q_values - returns) ** 2).mean()\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(q_net)\n",
    "    optimizer.update(grads)\n",
    "\n",
    "    return loss\n",
    "\n",
    "train_step(q_net, optimizer, obs_buffer, actions_buffer, returns)\n",
    "state = init_state(rng_key, config)\n",
    "q_net(state.board.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q_net: Model, optimizer: nnx.Optimizer, config: EnvConfig, params: Params):\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    losses = []\n",
    "    for iteration in range(params.num_iterations):\n",
    "        rng_key, rollout_key = jax.random.split(rng_key)\n",
    "        rollout = single_rollout(rollout_key, config, q_net, params)\n",
    "        obs_buffer, actions_buffer, rewards_buffer, done_buffer, next_obs_buffer, cum_return = rollout\n",
    "\n",
    "        returns = q_lambda_return(q_net, rewards_buffer, done_buffer, next_obs_buffer, params)\n",
    "\n",
    "        for epoch in range(params.update_epochs):\n",
    "            loss = train_step(q_net, optimizer, obs_buffer, actions_buffer, returns)\n",
    "            losses.append(loss)\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration} - Loss: {loss}\")\n",
    "\n",
    "    # evaluate\n",
    "    state = init_state(rng_key, config)\n",
    "    q_net(state.board.flatten())\n",
    "\n",
    "    plt.plot([min(l, 1000) for l in losses])\n",
    "\n",
    "    return q_net\n",
    "\n",
    "q_net = train(q_net, optimizer, config, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 10\n",
    "height = 10\n",
    "config = EnvConfig(\n",
    "    board_width = width,\n",
    "    board_height = height,\n",
    "    num_neutral_bases = 3,\n",
    "    num_neutral_troops_start = 5,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "params = Params(\n",
    "    num_iterations=5000,\n",
    "    lr=6e-4,\n",
    "    gamma=0.99,\n",
    "    q_lambda=0.65,\n",
    "    num_envs=2048,\n",
    "    num_steps=26,\n",
    "    update_epochs=4,\n",
    "    num_minibatches=4,\n",
    "    epsilon=0.4,\n",
    ")\n",
    "q_net = Model(width*height*4, 256, width*height*4, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(q_net, optax.adam(params.lr))\n",
    "\n",
    "q_net = train(q_net, optimizer, config, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the trained model to play a game\n",
    "state = init_state(rng_key, config)\n",
    "for i in range(500):\n",
    "    legal_mask = get_legal_moves(state, 0)\n",
    "    legal_mask = jnp.array(legal_mask.flatten())\n",
    "    action = jnp.argmax((q_net(state.board.flatten()) + 10) * legal_mask)\n",
    "    # split action from int to array\n",
    "    # y, x, direction\n",
    "    action = jnp.array([action // (width*4), (action % (width*4))//4, action % 4])\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state, p1_reward = p1_step(state, subkey, config, action)\n",
    "    assert_valid_state(state)\n",
    "    if i % 5 == 0:\n",
    "        visualize_board(state)\n",
    "    print(p1_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
