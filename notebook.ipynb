{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import functools\n",
    "import os\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rts.config import EnvConfig\n",
    "from src.rts.env import Board, EnvState, init_state, random_step, p1_step\n",
    "from src.rts.utils import assert_valid_state, get_legal_moves\n",
    "from src.rts.visualizaiton import visualize_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAX_CHECK_TRACER_LEAKS\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a game with random moves for 1000 steps\n",
    "# Visualize the board interactivly\n",
    "rng_key = jax.random.PRNGKey(3)\n",
    "config = EnvConfig(\n",
    "    board_width = 10,\n",
    "    board_height = 10,\n",
    "    num_neutral_bases = 6,\n",
    "    num_neutral_troops_start = 10,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "state = init_state(rng_key, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state = random_step(state, subkey, config)\n",
    "    assert_valid_state(state)\n",
    "    if i % 1 == 0:\n",
    "        visualize_board(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=(\"config\",))\n",
    "def batched_step(states, rng_keys, config):\n",
    "    def single_step(state, key):\n",
    "        return random_step(state, key, config)\n",
    "\n",
    "    return jax.vmap(single_step)(states, rng_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we vmap\n",
    "N = 50\n",
    "rng_key = jax.random.PRNGKey(3)\n",
    "rng_keys = jax.random.split(rng_key, N)\n",
    "\n",
    "# Create the initial state for each game via vmap.\n",
    "batched_init_state = jax.vmap(lambda key: init_state(key, config))\n",
    "states = batched_init_state(rng_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    # For each parallel game, split its RNG key into two:\n",
    "    # keys_split will have shape (N, 2, key_shape).\n",
    "    keys_split = jax.vmap(lambda key: jax.random.split(key, 2))(rng_keys)\n",
    "    # Update rng_keys to the first half and use the second half as subkeys.\n",
    "    rng_keys = keys_split[:, 0]\n",
    "    subkeys = keys_split[:, 1]\n",
    "\n",
    "    # Take one step in parallel for all games.\n",
    "    states = batched_step(states, subkeys, config)\n",
    "\n",
    "    # Visualize and validate\n",
    "    if i % 250 == 0:\n",
    "        board = Board(\n",
    "            player_1_troops = states.board.player_1_troops[79],\n",
    "            player_2_troops = states.board.player_2_troops[79],\n",
    "            neutral_troops = states.board.neutral_troops[79],\n",
    "            bases = states.board.bases[79],\n",
    "        )\n",
    "        single_state = EnvState(board = board)\n",
    "        assert_valid_state(single_state)\n",
    "        visualize_board(single_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "from src.rl.pqn import Model\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(3)\n",
    "width = 10\n",
    "height = 10\n",
    "model = Model(width*height*4, 256, width*height*4, rngs=nnx.Rngs(0))\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EnvConfig(\n",
    "    board_width = width,\n",
    "    board_height = height,\n",
    "    num_neutral_bases = 3,\n",
    "    num_neutral_troops_start = 5,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "\n",
    "state = init_state(rng_key, config)\n",
    "for i in range(50):\n",
    "    flat_state = jnp.array(state.board.flatten())\n",
    "    legal_mask = get_legal_moves(state, 0)\n",
    "    legal_mask = jnp.array(legal_mask.flatten())\n",
    "    action = jnp.argmax((model(flat_state) + 10) * legal_mask)\n",
    "    # split action from int to array\n",
    "    # y, x, direction\n",
    "    action = jnp.array([action // (width*4), (action % (width*4))//4, action % 4])\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state, p1_reward = p1_step(state, subkey, config, action)\n",
    "    assert_valid_state(state)\n",
    "    if i % 5 == 0:\n",
    "        visualize_board(state)\n",
    "    print(p1_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rl.pqn import Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_state = state.board.flatten()\n",
    "legal_mask = get_legal_moves(state, 0).flatten()\n",
    "q_net_action = jnp.argmax((model(flat_state) + 1000) * legal_mask)\n",
    "q_net_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rl.pqn import single_rollout\n",
    "params = Params(\n",
    "    num_iterations=100,\n",
    "    lr=6e-4,\n",
    "    gamma=0.99,\n",
    "    q_lambda=0.65,\n",
    "    num_envs=2048,\n",
    "    num_steps=20,\n",
    "    update_epochs=4,\n",
    "    num_minibatches=4,\n",
    "    epsilon=0.25,\n",
    ")\n",
    "q_net = Model(width*height*4, 256, width*height*4, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(q_net, optax.adam(params.lr))\n",
    "\n",
    "obs_buffer, actions_buffer, rewards_buffer, done_buffer, next_obs_buffer, cum_return = single_rollout(rng_key, config, q_net, params)\n",
    "actions_buffer, done_buffer, rewards_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rl.pqn import q_lambda_return\n",
    "\n",
    "returns = q_lambda_return(q_net, rewards_buffer, done_buffer, next_obs_buffer, params)\n",
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rl.pqn import train_step\n",
    "\n",
    "train_step(q_net, optimizer, obs_buffer, actions_buffer, returns)\n",
    "\n",
    "print(jnp.take_along_axis(q_net(obs_buffer), actions_buffer[:, None], axis=1).squeeze())\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q_net: Model, optimizer: nnx.Optimizer, config: EnvConfig, params: Params):\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    losses = []\n",
    "    for iteration in range(params.num_iterations):\n",
    "        rng_key, rollout_key = jax.random.split(rng_key)\n",
    "        rollout = single_rollout(rollout_key, config, q_net, params)\n",
    "        obs_buffer, actions_buffer, rewards_buffer, done_buffer, next_obs_buffer, cum_return = rollout\n",
    "\n",
    "        returns = q_lambda_return(q_net, rewards_buffer, done_buffer, next_obs_buffer, params)\n",
    "\n",
    "        # print(returns)\n",
    "\n",
    "        for epoch in range(params.update_epochs):\n",
    "            loss = train_step(q_net, optimizer, obs_buffer, actions_buffer, returns)\n",
    "            losses.append(loss)\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration} - Loss: {loss}\")\n",
    "\n",
    "    # evaluate\n",
    "    state = init_state(rng_key, config)\n",
    "    q_net(state.board.flatten())\n",
    "\n",
    "    plt.plot([min(l, 1000) for l in losses])\n",
    "\n",
    "    return q_net\n",
    "\n",
    "q_net = train(q_net, optimizer, config, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 10\n",
    "height = 10\n",
    "config = EnvConfig(\n",
    "    board_width = width,\n",
    "    board_height = height,\n",
    "    num_neutral_bases = 3,\n",
    "    num_neutral_troops_start = 5,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "params = Params(\n",
    "    num_iterations=500,\n",
    "    lr=4e-4,\n",
    "    gamma=0.99,\n",
    "    q_lambda=0.92,\n",
    "    num_envs=50,\n",
    "    num_steps=250,\n",
    "    update_epochs=1,\n",
    "    num_minibatches=4,\n",
    "    epsilon=0.3,\n",
    ")\n",
    "q_net = Model(width*height*4, 256, width*height*4, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(q_net, optax.adam(params.lr))\n",
    "\n",
    "q_net = train(q_net, optimizer, config, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the trained model to play a game\n",
    "state = init_state(rng_key, config)\n",
    "rewards = []\n",
    "for i in range(200):\n",
    "    legal_mask = get_legal_moves(state, 0)\n",
    "    legal_mask = jnp.array(legal_mask.flatten())\n",
    "    print(q_net(state.board.flatten())* legal_mask)\n",
    "    action = jnp.argmax((q_net(state.board.flatten()) + 1000) * legal_mask)\n",
    "    # split action from int to array\n",
    "    # y, x, direction\n",
    "    action = jnp.array([action // (width*4), (action % (width*4))//4, action % 4])\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    state, p1_reward = p1_step(state, subkey, config, action)\n",
    "    rewards.append(p1_reward)\n",
    "    assert_valid_state(state)\n",
    "    if i % 5 == 0:\n",
    "        visualize_board(state)\n",
    "    print(p1_reward)\n",
    "print(rewards)\n",
    "# for each step print the cumulative reward to the end from that step\n",
    "print(np.cumsum(rewards))\n",
    "plt.plot(np.cumsum(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PQN with vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_keys = jax.random.split(rng_key, params.num_envs)\n",
    "\n",
    "vmapped_rollout = jax.vmap(single_rollout, in_axes=(0, None, None, None))\n",
    "\n",
    "obs_buffer, actions_buffer, rewards_buffer, done_buffer, next_obs_buffer, cum_returns = vmapped_rollout(\n",
    "    rng_keys, config, q_net, params\n",
    ")\n",
    "obs_buffer.shape, actions_buffer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmapped_q_lambda_return = jax.vmap(q_lambda_return, in_axes=(None, 0, 0, 0, None))\n",
    "\n",
    "returns = vmapped_q_lambda_return(q_net, rewards_buffer, done_buffer, next_obs_buffer, params)\n",
    "returns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the first two dimensions (envs and time steps)\n",
    "flat_observations = obs_buffer.reshape(-1, obs_buffer.shape[-1])\n",
    "flat_actions = actions_buffer.reshape(-1)\n",
    "flat_returns = returns.reshape(-1)\n",
    "\n",
    "# Now train your single model on all the data in one go.\n",
    "loss = train_step(q_net, optimizer, flat_observations, flat_actions, flat_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vmapped(q_net: Model, optimizer: nnx.Optimizer, config: EnvConfig, params: Params):\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    vmapped_rollout = jax.vmap(single_rollout, in_axes=(0, None, None, None))\n",
    "    vmapped_q_lambda_return = jax.vmap(q_lambda_return, in_axes=(None, 0, 0, 0, None))\n",
    "    losses = []\n",
    "    for iteration in range(params.num_iterations):\n",
    "        rng_keys = jax.random.split(rng_key, params.num_envs + 1)\n",
    "        rng_key, rollout_keys = rng_keys[0], rng_keys[1:]\n",
    "        rollout = vmapped_rollout(rollout_keys, config, q_net, params)\n",
    "        obs_buffer, actions_buffer, rewards_buffer, done_buffer, next_obs_buffer, cum_return = rollout\n",
    "\n",
    "        returns = vmapped_q_lambda_return(q_net, rewards_buffer, done_buffer, next_obs_buffer, params)\n",
    "\n",
    "        flat_observations = obs_buffer.reshape(-1, obs_buffer.shape[-1])\n",
    "        flat_actions = actions_buffer.reshape(-1)\n",
    "        flat_returns = returns.reshape(-1)\n",
    "\n",
    "        loss = train_step(q_net, optimizer, flat_observations, flat_actions, flat_returns)\n",
    "        losses.append(loss)\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration} - Loss: {loss}\")\n",
    "\n",
    "    plt.plot([min(l, 1000) for l in losses])\n",
    "\n",
    "    return q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 10\n",
    "height = 10\n",
    "config = EnvConfig(\n",
    "    board_width = width,\n",
    "    board_height = height,\n",
    "    num_neutral_bases = 3,\n",
    "    num_neutral_troops_start = 5,\n",
    "    neutral_troops_min = 4,\n",
    "    neutral_troops_max = 10,\n",
    "    player_start_troops=5,\n",
    "    bonus_time=10,\n",
    ")\n",
    "params = Params(\n",
    "    num_iterations=500,\n",
    "    lr=4e-4,\n",
    "    gamma=0.99,\n",
    "    q_lambda=0.92,\n",
    "    num_envs=50,\n",
    "    num_steps=250,\n",
    "    update_epochs=1,\n",
    "    num_minibatches=4,\n",
    "    epsilon=0.3,\n",
    ")\n",
    "q_net = Model(width*height*4, 256, width*height*4, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(q_net, optax.adam(params.lr))\n",
    "\n",
    "q_net = train_vmapped(q_net, optimizer, config, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatched(q_net: Model, optimizer: nnx.Optimizer, config: EnvConfig, params: Params):\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    vmapped_rollout = jax.vmap(single_rollout, in_axes=(0, None, None, None))\n",
    "    vmapped_q_lambda_return = jax.vmap(q_lambda_return, in_axes=(None, 0, 0, 0, None))\n",
    "    losses = []\n",
    "    \n",
    "    for iteration in range(params.num_iterations):\n",
    "        # Split rng_key for each environment.\n",
    "        rng_keys = jax.random.split(rng_key, params.num_envs + 1)\n",
    "        rng_key, rollout_keys = rng_keys[0], rng_keys[1:]\n",
    "        \n",
    "        # Run vmapped rollout across all environments.\n",
    "        rollout = vmapped_rollout(rollout_keys, config, q_net, params)\n",
    "        obs_buffer, actions_buffer, rewards_buffer, done_buffer, next_obs_buffer, cum_return = rollout\n",
    "\n",
    "        # Compute returns using vmapped q_lambda_return.\n",
    "        returns = vmapped_q_lambda_return(q_net, rewards_buffer, done_buffer, next_obs_buffer, params)\n",
    "\n",
    "        # Flatten rollout buffers to combine envs and timesteps into one batch dimension.\n",
    "        flat_observations = obs_buffer.reshape(-1, obs_buffer.shape[-1])\n",
    "        flat_actions = actions_buffer.reshape(-1)\n",
    "        flat_returns = returns.reshape(-1)\n",
    "\n",
    "        num_samples = flat_observations.shape[0]\n",
    "        minibatch_size = num_samples // params.num_minibatches\n",
    "\n",
    "        # Perform two passes (epochs) over the flattened rollout data.\n",
    "        for epoch in range(params.update_epochs):\n",
    "            # Shuffle indices for minibatch splitting.\n",
    "            rng_key, perm_key = jax.random.split(rng_key)\n",
    "            permuted_indices = jax.random.permutation(perm_key, num_samples)\n",
    "            \n",
    "            for i in range(params.num_minibatches):\n",
    "                start_idx = i * minibatch_size\n",
    "                # Ensure that the last minibatch gets any remaining samples.\n",
    "                end_idx = (i + 1) * minibatch_size if i < params.num_minibatches - 1 else num_samples\n",
    "                minibatch_idx = permuted_indices[start_idx:end_idx]\n",
    "\n",
    "                minibatch_obs = flat_observations[minibatch_idx]\n",
    "                minibatch_actions = flat_actions[minibatch_idx]\n",
    "                minibatch_returns = flat_returns[minibatch_idx]\n",
    "\n",
    "                loss = train_step(q_net, optimizer, minibatch_obs, minibatch_actions, minibatch_returns)\n",
    "                losses.append(loss)\n",
    "\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration} - Loss: {loss}\")\n",
    "\n",
    "    plt.plot([min(l, 1000) for l in losses])\n",
    "    return q_net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
